{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f34494bb",
   "metadata": {},
   "source": [
    "# Identify and rerun failed Sentinel-2 jobs\n",
    "\n",
    "Some Sentinel-2 images are inaccessible from GEE. When jobs are submitted with monthly time ranges, for example, one inaccesible image can lead to an entire month of lost observations. \n",
    "\n",
    "The goals of this notebook therefore are to:\n",
    "- Load your RGI glaciers subset\n",
    "- Load compiled time series output from `compile_CSVs.ipynb` and downloaded locally\n",
    "\n",
    "Then, for each glacier:\n",
    "- Identify time ranges with no Sentinel-2 observations\n",
    "- Rerun the pipeline for the missing Sentinel-2 months with daily time ranges.\n",
    "\n",
    "I recommend NOT downloading any new files locally until each batch of failed jobs has been successfully resubmitted to keep indexing, etc. consistent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a93612a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ee \n",
    "import sys\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -----Define path in local directory to compiled time series\n",
    "ts_folder = '/Users/rdcrlrka/Research/glacier_snow_mapping/glacier_snow_cover_exports_compiled'\n",
    "\n",
    "# -----Define Google Drive folder for exports\n",
    "# NOTE: Make sure this folder already exists and is the only folder with that name in \"My Drive\". \n",
    "out_folder = 'glacier_snow_cover_exports'\n",
    "\n",
    "# -----Import pipeline utilities\n",
    "# Assumes pipeline_utils.py is in the same folder as this notebook\n",
    "script_path = os.getcwd()\n",
    "sys.path.append(script_path)\n",
    "import glasee_pipeline_utils as utils\n",
    "\n",
    "# -----Define image search settings\n",
    "# Date and month ranges (inclusive)\n",
    "date_start = '2014-04-01' \n",
    "date_end = '2025-09-15' \n",
    "month_start = 4 # April = 4\n",
    "month_end = 10 # Oct = 10\n",
    "# Minimum fill portion percentage of the AOI (0–100), used to remove images after mosaicking by day\n",
    "min_aoi_coverage = 70\n",
    "# Whether to mask clouds using the respective cloud mask via the geedim package\n",
    "mask_clouds = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ade75d",
   "metadata": {},
   "source": [
    "## Authenticate and/or Initialize Google Earth Engine (GEE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7727ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"ee-raineyaberle\"\n",
    "\n",
    "try:\n",
    "    ee.Initialize(project=project_id)\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(project=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d151d13d",
   "metadata": {},
   "source": [
    "## Load the RGI glaciers subset and compiled time series file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f4ac39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of glaciers selected from the RGI: 149\n",
      "Number of glaciers with compiled time series: 144\n",
      "Number of glaciers in the RGI subset without compiled time series: 5\n",
      "['RGI2000-v7.0-G-02-05381', 'RGI2000-v7.0-G-02-10331', 'RGI2000-v7.0-G-02-10336', 'RGI2000-v7.0-G-02-10357', 'RGI2000-v7.0-G-02-10372']\n"
     ]
    }
   ],
   "source": [
    "# Load the RGI v. 7 dataset\n",
    "rgi = ee.FeatureCollection(\"projects/ee-raineyaberle/assets/glacier-snow-cover-mapping/RGI2000-v7-G\")\n",
    "# Apply filters\n",
    "rgi_filt = (rgi\n",
    "            .filter(ee.Filter.eq('o1region', '02')) # Western U.S. & Canada\n",
    "            .filter(ee.Filter.gte('area_km2', 10)) # area > 10 km2\n",
    "            .filter(ee.Filter.lte('area_km2', 200)) # area < 200 km2\n",
    "            )\n",
    "# Get the list of RGI IDs\n",
    "id_list = rgi_filt.aggregate_array('rgi_id')\n",
    "id_list = id_list.getInfo()\n",
    "print('Number of glaciers selected from the RGI:', len(id_list))\n",
    "\n",
    "# Load compiled time series file names\n",
    "ts_files = sorted(glob(os.path.join(ts_folder, '*.csv')))\n",
    "ts_id_list = [os.path.basename(x).split('_')[0] for x in ts_files]\n",
    "print(\"Number of glaciers with compiled time series:\", len(ts_id_list))\n",
    "\n",
    "# Identify glaciers in the RGI subset without compiled time series\n",
    "missing_id_list = [x for x in id_list if x not in ts_id_list]\n",
    "print('Number of glaciers in the RGI subset without compiled time series:', len(missing_id_list))\n",
    "if missing_id_list:\n",
    "    print(missing_id_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1e438d",
   "metadata": {},
   "source": [
    "## Identify months with no observations for each glacier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "759e4fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 144/144 [00:00<00:00, 289.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glaciers with missing months: 144\n",
      "Total number of missing months across all glaciers: 1339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Intialize dictionary to hold results\n",
    "missing_dict = {}\n",
    "\n",
    "# Iterate over study sites\n",
    "for rgi_id in tqdm(ts_id_list):\n",
    "    # open the compiled time series\n",
    "    ts_file = [x for x in ts_files if rgi_id in os.path.basename(x)][0]\n",
    "    ts = pd.read_csv(ts_file)\n",
    "    ts['date'] = pd.to_datetime(ts['date'])\n",
    "\n",
    "    # identify missing Sentinel-2_SR months\n",
    "    ts_s2_sr = ts.loc[ts['source']==\"Sentinel-2_SR\"]\n",
    "    all_months = pd.date_range(start=\"2019-01-01\", end=date_end, freq='MS')\n",
    "    all_months = all_months[(all_months.month >= month_start) & (all_months.month <= month_end)]\n",
    "    ts_months = ts_s2_sr['date'].dt.to_period('M').drop_duplicates().dt.to_timestamp()\n",
    "    missing_months_s2_sr = all_months.difference(ts_months)\n",
    "\n",
    "    # identify missing Sentinel-2_TOA months\n",
    "    ts_s2_toa = ts.loc[ts['source']==\"Sentinel-2_TOA\"]\n",
    "    all_months = pd.date_range(start=\"2016-05-01\", end=date_end, freq='MS')\n",
    "    all_months = all_months[(all_months.month >= month_start) & (all_months.month <= month_end)]\n",
    "    ts_months = ts_s2_toa['date'].dt.to_period('M').drop_duplicates().dt.to_timestamp()\n",
    "    missing_months_s2_toa = all_months.difference(ts_months)\n",
    "    \n",
    "    # add to the dictionary\n",
    "    missing_dict[rgi_id] = {\n",
    "        'Sentinel-2_SR': [x.strftime('%Y-%m') for x in missing_months_s2_sr],\n",
    "        'Sentinel-2_TOA': [x.strftime('%Y-%m') for x in missing_months_s2_toa],\n",
    "    }\n",
    "\n",
    "total_missing_months = sum([len(missing_dict[x]['Sentinel-2_SR']) + len(missing_dict[x]['Sentinel-2_TOA']) for x in missing_dict])\n",
    "\n",
    "print('Glaciers with missing months:', len(missing_dict.keys()))\n",
    "print('Total number of missing months across all glaciers:', total_missing_months)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad531bc",
   "metadata": {},
   "source": [
    "## Rerun failed jobs with daily time ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d0982fa-b119-4aea-b607-bab17fe4b683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def run_daily_jobs(dataset, missing_months, rgi_id, aoi, dem, out_folder):\n",
    "    # determine scale\n",
    "    scale = 10 if ('Sentinel-2' in dataset) else 30\n",
    "    \n",
    "    # iterate over months\n",
    "    for month in missing_months:\n",
    "        print(month)\n",
    "        \n",
    "        # create daily time ranges\n",
    "        days = pd.date_range(\n",
    "            month + '-01', \n",
    "            f\"{month[0:4]}-{int(month[-2:])+1}-01\"\n",
    "        )\n",
    "        days = [str(x)[0:10] for x in days]\n",
    "        day_ranges = list(zip(days[0:-1], days[1:]))\n",
    "        \n",
    "        # iterate over day ranges\n",
    "        for day_range in tqdm(day_ranges):\n",
    "            # Query GEE for imagery\n",
    "            image_collection = utils.query_gee_for_imagery(\n",
    "                dataset, aoi, day_range[0], day_range[1], month_start, month_end, \n",
    "                min_aoi_coverage, mask_clouds, scale, verbose=False\n",
    "            )\n",
    "        \n",
    "            # Classify image collection\n",
    "            classified_collection = utils.classify_image_collection(image_collection, dataset, verbose=False)\n",
    "    \n",
    "            # Calculate snow cover statistics, export to Google Drive\n",
    "            _ = utils.calculate_snow_cover_statistics(\n",
    "                classified_collection, dem, aoi, dataset, scale, out_folder,\n",
    "                file_name_prefix=f\"{rgi_id}_{dataset}_snow_cover_stats_{day_range[0]}_{day_range[1]}\", verbose=False\n",
    "            )\n",
    "    return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5db3c392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for rgi_id in list(missing_dict.keys())[147:]:\n",
    "    print(f'\\n{rgi_id}')\n",
    "    print('----------------')\n",
    "    # Get the AOI\n",
    "    aoi = rgi.filter(ee.Filter.eq('rgi_id', rgi_id)).geometry()\n",
    "\n",
    "    # Query GEE for DEM\n",
    "    dem = utils.query_gee_for_dem(aoi) \n",
    "    \n",
    "    # Sentinel-2_SR\n",
    "    # print('\\nSentinel-2_SR')\n",
    "    # missing_months_s2_sr = missing_dict[rgi_id]['Sentinel-2_SR']\n",
    "    # run_daily_jobs(\"Sentinel-2_SR\", missing_months_s2_sr, rgi_id, aoi, dem, out_folder)\n",
    "        \n",
    "    # Sentinel-2_TOA\n",
    "    print('\\nSentinel-2_TOA')\n",
    "    missing_months_s2_toa = missing_dict[rgi_id]['Sentinel-2_TOA']\n",
    "    run_daily_jobs(\"Sentinel-2_TOA\", missing_months_s2_toa, rgi_id, aoi, dem, out_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "344f16f8-61f7-4cd4-87d8-b9068f389be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tasks = 18831\n",
      "RUNNING tasks = 0\n",
      "READY tasks = 1443\n",
      "COMPLETED tasks = 17326\n",
      "FAILED tasks = 62\n"
     ]
    }
   ],
   "source": [
    "# Check job queue statuses\n",
    "tasks = ee.batch.Task.list()\n",
    "running_count = len([x for x in tasks if x.state=='RUNNING'])\n",
    "ready_count = len([x for x in tasks if x.state=='READY'])\n",
    "completed_count = len([x for x in tasks if x.state=='COMPLETED'])\n",
    "failed_count = len([x for x in tasks if x.state=='FAILED'])\n",
    "\n",
    "print('Total number of tasks =', len(tasks))\n",
    "print('RUNNING tasks =', running_count)\n",
    "print('READY tasks =', ready_count)\n",
    "print('COMPLETED tasks =', completed_count)\n",
    "print('FAILED tasks =', failed_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "88e9c13d-679b-4949-b7d7-b5ab2b7673f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Job limit exceeded, whoops\n",
    "# rgi_id = 'RGI2000-v7.0-G-02-08338'\n",
    "# dataset = 'Sentinel-2_TOA'\n",
    "# # create daily time ranges\n",
    "# days = pd.date_range('2020-10-19', '2020-11-01')\n",
    "# days = [str(x)[0:10] for x in days]\n",
    "# day_ranges = list(zip(days[0:-1], days[1:]))\n",
    "# scale = 10\n",
    "\n",
    "#  # iterate over day ranges\n",
    "# for day_range in tqdm(day_ranges):\n",
    "#     # Query GEE for imagery\n",
    "#     image_collection = utils.query_gee_for_imagery(\n",
    "#         dataset, aoi, day_range[0], day_range[1], month_start, month_end, \n",
    "#         min_aoi_coverage, mask_clouds, scale, verbose=False\n",
    "#     )\n",
    "\n",
    "#     # Classify image collection\n",
    "#     classified_collection = utils.classify_image_collection(image_collection, dataset, verbose=False)\n",
    "\n",
    "#     # Calculate snow cover statistics, export to Google Drive\n",
    "#     _ = utils.calculate_snow_cover_statistics(\n",
    "#         classified_collection, dem, aoi, dataset, scale, out_folder,\n",
    "#         file_name_prefix=f\"{rgi_id}_{dataset}_snow_cover_stats_{day_range[0]}_{day_range[1]}\", verbose=False\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f2c018-6b7a-4846-8927-b160fa71db0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
