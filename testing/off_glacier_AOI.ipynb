{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "679c57e8",
   "metadata": {},
   "source": [
    "# Classify snow for an off-glacier site using the NDSI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818d3278",
   "metadata": {},
   "source": [
    "## Define settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d472ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wxee (used to convert an ee.Image to xarray.Dataset) is not included in the default environment. \n",
    "# Install by uncommenting the line below.\n",
    "# !micromamba install -c conda-forge wxee -y\n",
    "\n",
    "import os\n",
    "import ee\n",
    "import sys\n",
    "import wxee as wx\n",
    "import numpy as np\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "\n",
    "# --- Define options for downloading outputs ---\n",
    "out_folder = '/Users/rdcrlrka/Research/glacier_snow_mapping/LemonCreek_watershed_AOI'\n",
    "download_images = True\n",
    "plot_results = True\n",
    "\n",
    "# --- Define image search and classification settings ---\n",
    "# Date and month ranges (inclusive)\n",
    "date_start = '2017-04-01' \n",
    "date_end = '2017-11-01' \n",
    "month_start = 4 # April = 4\n",
    "month_end = 10 # Oct = 10\n",
    "# Minimum fill portion percentage of the AOI (0â€“100), used to remove images after mosaicking by day\n",
    "min_aoi_coverage = 70\n",
    "# Whether to mask clouds using the respective cloud mask via the geedim package\n",
    "mask_clouds = True\n",
    "# NDSI threshold used to classify snow (set to None to skip)\n",
    "ndsi_threshold = 0.6\n",
    "# Blue band threshold used to classify snow (set to None to skip)\n",
    "blue_threshold = 0.4\n",
    "# SLA percentile: the percentile of snow-covered elevations to sample, from 0 to 100\n",
    "sla_percentile = 5\n",
    "\n",
    "# --- Import pipeline utilities ---\n",
    "# Assumes glasee_pipeline_utils.py is one folder above this notebook\n",
    "script_path = os.getcwd()\n",
    "sys.path.append(os.path.join(script_path, '..'))\n",
    "import glasee_pipeline_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fe38a",
   "metadata": {},
   "source": [
    "## Authenticate and/or Initialize Google Earth Engine (GEE)\n",
    "\n",
    "Replace the project ID with your GEE project. Default = `ee-[GEE-username]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c276471a",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = \"ee-raineyaberle\"\n",
    "\n",
    "try:\n",
    "    ee.Initialize(project=project_id)\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(project=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730e1cf",
   "metadata": {},
   "source": [
    "## Define AOI and query GEE for DEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Manual AOI\n",
    "# save just the coordinates first for later use\n",
    "aoi_coords = [\n",
    "[-134.379943, 58.417939],\n",
    "[-134.362549, 58.388341],\n",
    "[-134.361008, 58.3727101],\n",
    "[-134.369808, 58.359844],\n",
    "[-134.470524, 58.3375671],\n",
    "[-134.558482, 58.381999],\n",
    "[-134.379943, 58.41793911]\n",
    "]\n",
    "# convert to ee.Geometry\n",
    "aoi = ee.Geometry.Polygon(aoi_coords)\n",
    "aoi_area = aoi.area().getInfo()\n",
    "print(f\"AOI area = {int(aoi_area/1e6)} km2\")\n",
    "\n",
    "# -----Identify the best UTM zone for outputs\n",
    "def convert_wgs_to_utm(lon: float, lat: float):\n",
    "    utm_band = str(int((np.floor((lon + 180) / 6) % 60) + 1))\n",
    "    if len(utm_band) == 1:\n",
    "        utm_band = '0' + utm_band\n",
    "    if lat >= 0:\n",
    "        epsg_code = 'EPSG:326' + utm_band\n",
    "        return epsg_code\n",
    "    epsg_code = 'EPSG:327' + utm_band\n",
    "    return epsg_code\n",
    "\n",
    "aoi_cen_lon = float(np.nanmean(np.array(aoi_coords)[:,0]))\n",
    "aoi_cen_lat = float(np.nanmean(np.array(aoi_coords)[:,1]))\n",
    "utm_crs = convert_wgs_to_utm(aoi_cen_lon, aoi_cen_lat)\n",
    "print(\"Optimal UTM zone =\", utm_crs)\n",
    "\n",
    "# -----Query GEE for DEM\n",
    "dem = utils.query_gee_for_dem(aoi)\n",
    "# set an arbitrary time for DEM, otherwise wxee will get angry\n",
    "print(\"Downloading DEM as xarray.Dataset\")\n",
    "dem = dem.set('system:time_start', 0)\n",
    "dem_xr = dem.wx.to_xarray(region=aoi, scale=10, crs=utm_crs)\n",
    "# remove unnecessary dimensions\n",
    "dem_xr = dem_xr.squeeze(drop=True)\n",
    "if 'elevation' in dem_xr.data_vars:\n",
    "    dem_xr = dem_xr['elevation']\n",
    "\n",
    "# -----Plot\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "dem_xr.plot(ax=ax, cmap='terrain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1a332d-7df7-4af6-a5c1-e4d3b2074597",
   "metadata": {},
   "source": [
    "## Define some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6234e590-2ff9-48e2-b8f2-0bdc9bba228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_date_range_daily(\n",
    "        dataset: str = None, \n",
    "        date_start: str = None, \n",
    "        date_end: str = None, \n",
    "        month_start: int = None, \n",
    "        month_end: int = None\n",
    "        ):\n",
    "    # Convert string inputs to datetime objects\n",
    "    date_start = datetime.datetime.strptime(date_start, \"%Y-%m-%d\").date()\n",
    "    date_end = datetime.datetime.strptime(date_end, \"%Y-%m-%d\").date()\n",
    "\n",
    "    # Enforce dataset availability\n",
    "    dataset_start_years = {\n",
    "        \"Sentinel-2_TOA\": 2016,\n",
    "        \"Sentinel-2_SR\": 2019,\n",
    "        \"Landsat\": 2013,\n",
    "    }\n",
    "\n",
    "    if dataset not in dataset_start_years:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset}\")\n",
    "\n",
    "    min_year = dataset_start_years[dataset]\n",
    "    date_start = max(date_start, datetime.date(min_year, 1, 1))  # Clamp to dataset availability\n",
    "\n",
    "    # List to hold date range tuples\n",
    "    date_ranges = []\n",
    "    current = max(date_start, datetime.date(date_start.year, month_start, 1))\n",
    "    while current < date_end:\n",
    "        if month_start <= current.month <= month_end:\n",
    "            date_ranges.append((current.isoformat(), (current + datetime.timedelta(days=1)).isoformat()))\n",
    "        current += datetime.timedelta(days=1)\n",
    "\n",
    "    print(f\"Number of dates to query = {len(date_ranges)}\")\n",
    "\n",
    "    return date_ranges\n",
    "\n",
    "\n",
    "def classify_snow(\n",
    "        image_xr: xr.Dataset = None, \n",
    "        ndsi_threshold: float = None, \n",
    "        blue_threshold: float = None,\n",
    "        out_file: str = None\n",
    "        ):\n",
    "    \n",
    "    # Get the name of the blue band\n",
    "    if \"B2\" in image_xr.data_vars:\n",
    "        blue_band = \"B2\"\n",
    "    elif \"SR_B2\" in image_xr.data_vars:\n",
    "        blue_band = \"SR_B2\"\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot determine blue band for the image.\")\n",
    "\n",
    "    # Apply the thresholds specified\n",
    "    if (type(ndsi_threshold)==float) & (type(blue_threshold)==float):\n",
    "        image_classified = xr.where(\n",
    "            (image_xr.NDSI >= ndsi_threshold) \n",
    "            & (image_xr[blue_band] >= blue_threshold), 1, 0\n",
    "            )\n",
    "    elif (type(ndsi_threshold)==float):\n",
    "        image_classified = xr.where(image_xr.NDSI >= ndsi_threshold, 1, 0)\n",
    "    elif (type(blue_threshold)==float):\n",
    "        image_classified = xr.where(image_xr[blue_band] >= blue_threshold, 1, 0)\n",
    "    else:\n",
    "        raise ValueError(\"No thresholds provided to classify snow, cannot proceed.\")\n",
    "\n",
    "    # Put nodata values back in\n",
    "    image_classified = xr.where(np.isnan(image_xr.NDSI), np.nan, image_classified)\n",
    "\n",
    "    # Set raster attributes\n",
    "    image_classified = (\n",
    "        image_classified\n",
    "            .rio.write_crs(image_xr.rio.crs)\n",
    "            .rio.write_nodata(np.nan)\n",
    "            )\n",
    "    \n",
    "    # Save to file\n",
    "    if type(out_file)==str:\n",
    "        # convert to int datatype\n",
    "        image_classified_int = xr.where(np.isnan(image_classified), 9999, image_classified).astype(int)\n",
    "        image_classified_int = (\n",
    "        image_classified_int\n",
    "            .rio.write_crs(image_xr.rio.crs)\n",
    "            .rio.write_nodata(9999)\n",
    "            )\n",
    "        image_classified_int.rio.to_raster(out_file, dtype=np.uint16)\n",
    "        print(f\"Classified image saved to: {out_file}\")\n",
    "\n",
    "    return image_classified\n",
    "\n",
    "def calculate_snowline_altitude(\n",
    "        date: str = None,\n",
    "        dataset: str = None,\n",
    "        classified_image: xr.Dataset = None, \n",
    "        dem: xr.DataArray = None, \n",
    "        sla_percentile: float = 5,\n",
    "        out_file: str = None\n",
    "    ):\n",
    "\n",
    "    # --- Calculate pixel area ---\n",
    "    scale = np.nanmean(classified_image.x.data[1:] - classified_image.x.data[0:-1])\n",
    "    pixel_area = float(scale) ** 2\n",
    "\n",
    "    # --- Calculate snow, no snow, and masked areas ---\n",
    "    snow_mask = (classified_image == 1)\n",
    "    snow_covered_area = np.sum(snow_mask).data * pixel_area\n",
    "\n",
    "    snow_free_mask = (classified_image == 0)\n",
    "    snow_free_area = np.sum(snow_free_mask).data * pixel_area\n",
    "\n",
    "    nodata_mask = np.isnan(classified_image)\n",
    "    nodata_area = np.sum(nodata_mask).data * pixel_area\n",
    "\n",
    "    # --- Mask DEM to where there are observations ---\n",
    "    # make sure DEM is on the same grid first\n",
    "    dem = dem.rio.reproject_match(classified_image)\n",
    "    dem_masked = dem.where(~np.isnan(classified_image))\n",
    "\n",
    "    # --- Estimate Snowline Altitude (SLA) ---\n",
    "    snow_dem = dem_masked.where(snow_mask)\n",
    "    sla = float(np.nanpercentile(snow_dem, sla_percentile))\n",
    "\n",
    "    # --- Estimate SLA upper and lower bounds ---\n",
    "    # \"Reference system switch\": find the DEM percentile corresponding to the SLA\n",
    "    below_sla_mask = dem < sla\n",
    "    below_sla_mask_area = float(below_sla_mask.sum()) * pixel_area\n",
    "    sla_percentile_dem = (below_sla_mask_area / aoi_area) * 100\n",
    "\n",
    "    # Upper bound\n",
    "    above_sla_mask = dem > sla\n",
    "    sla_upper_mask = snow_free_mask & above_sla_mask\n",
    "    sla_upper_mask_area = float(sla_upper_mask.sum()) * pixel_area\n",
    "    if sla_upper_mask_area > 0:\n",
    "        # DEM percentile to sample = (SLA percentile) + (Area snow-free above SLA / Total AOI Area)\n",
    "        sla_upper_percentile = sla_percentile_dem + (sla_upper_mask_area / aoi_area) * 100\n",
    "        # make sure it's between 0 and 1\n",
    "        sla_upper_percentile = np.clip(sla_upper_percentile/100, 0, 1)\n",
    "        sla_upper = float(dem.quantile(sla_upper_percentile, skipna=True))\n",
    "    else:\n",
    "        sla_upper = sla\n",
    "\n",
    "    # Lower bound\n",
    "    sla_lower_mask = (classified_image == 1) & below_sla_mask\n",
    "    sla_lower_mask_area = float(sla_lower_mask.sum()) * pixel_area\n",
    "    if sla_lower_mask_area > 0:\n",
    "        # DEM percentile to sample = (SLA percentile) - (Area snow-covered below SLA / Total AOI Area)\n",
    "        sla_lower_percentile = sla_percentile_dem - (sla_lower_mask_area / aoi_area) * 100\n",
    "        # make sure it's between 0 and 1\n",
    "        sla_lower_percentile = np.clip(sla_lower_percentile / 100, 0, 1)\n",
    "        sla_lower = float(dem.quantile(sla_lower_percentile, skipna=True))\n",
    "    else:\n",
    "        sla_lower = sla\n",
    "\n",
    "    # Compile results in a pd.DataFrame\n",
    "    # rounding floats because we're not that precise\n",
    "    df = pd.DataFrame({\n",
    "        \"date\": [date],\n",
    "        \"dataset\": [dataset],\n",
    "        \"snow_covered_area_m2\": [int(snow_covered_area)],\n",
    "        \"snow_free_area_m2\": [int(snow_free_area)],\n",
    "        \"masked_area_m2\": [int(nodata_area)],\n",
    "        \"SLA_m\": [int(sla)],\n",
    "        \"SLA_lower_bound_m\": [int(sla_lower)],\n",
    "        \"SLA_upper_bound_m\": [int(sla_upper)]\n",
    "    }, index=[0])\n",
    "\n",
    "    # Save to file\n",
    "    if type(out_file)==str:\n",
    "        df.to_csv(out_file, index=False)\n",
    "        print(f\"Snow cover stats saved to: {out_file}\")\n",
    "\n",
    "    return df, dem_masked\n",
    "\n",
    "def plot_snow_cover_stats(\n",
    "        date: str = None, \n",
    "        dataset: str = None,\n",
    "        image: xr.Dataset = None, \n",
    "        classified_image: xr.DataArray = None,\n",
    "        dem: xr.DataArray = None,\n",
    "        snow_stats_df: pd.DataFrame = None,\n",
    "        out_file: str = None\n",
    "    ):\n",
    "    plt.rcParams.update({\"font.size\": 12, \"font.sans-serif\": \"Verdana\"})\n",
    "\n",
    "    # Grab info from df\n",
    "    sla = snow_stats_df['SLA_m'].values[0]\n",
    "    sla_lower = snow_stats_df['SLA_lower_bound_m'].values[0]\n",
    "    sla_upper = snow_stats_df['SLA_upper_bound_m'].values[0]\n",
    "\n",
    "    # Set up figure\n",
    "    gs = matplotlib.gridspec.GridSpec(2, 2, height_ratios=[2,1])\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = [\n",
    "        fig.add_subplot(gs[0,0]),\n",
    "        fig.add_subplot(gs[0,1]),\n",
    "        fig.add_subplot(gs[1,:])\n",
    "    ]\n",
    "    # RGB image\n",
    "    if \"Sentinel-2\" in dataset:\n",
    "        rgb_bands = ['B4', 'B3', 'B2']\n",
    "    else:\n",
    "        rgb_bands = ['SR_B4', 'SR_B3', 'SR_B2']\n",
    "    ax[0].imshow(\n",
    "        np.dstack([image[rgb_bands[0]], image[rgb_bands[1]], image[rgb_bands[2]]]),\n",
    "        extent=(min(image.x)/1e3, max(image.x)/1e3, min(image.y)/1e3, max(image.y)/1e3)\n",
    "    )\n",
    "\n",
    "    ax[0].set_xlabel(\"Easting [km]\")\n",
    "    ax[0].set_ylabel(\"Northing [km]\")\n",
    "\n",
    "    # Classified image + SLA contour\n",
    "    colors = [\"w\", \"#2C98CA\"] \n",
    "    bounds = [0,1] \n",
    "    cmap = matplotlib.colors.ListedColormap(colors)\n",
    "    norm = matplotlib.colors.BoundaryNorm(bounds, cmap.N)\n",
    "    im = ax[1].imshow(\n",
    "        classified_image,\n",
    "        cmap=cmap,\n",
    "        norm=norm,\n",
    "        extent=(\n",
    "            min(classified_image.x)/1e3, max(classified_image.x)/1e3, \n",
    "            min(classified_image.y)/1e3, max(classified_image.y)/1e3\n",
    "        )\n",
    "    )\n",
    "    X,Y = np.meshgrid(dem.x.data, dem.y.data)\n",
    "    ax[1].contour(np.divide(X, 1e3), np.divide(Y, 1e3), dem.data, levels=[sla], colors='k', linestyles='solid')\n",
    "    # ax[1].contour(np.divide(X, 1e3), np.divide(Y, 1e3), dem.data, levels=[sla_upper], colors='k', linestyles='dashed')\n",
    "    # ax[1].contour(np.divide(X, 1e3), np.divide(Y, 1e3), dem.data, levels=[sla_lower], colors='k', linestyles='dotted')\n",
    "\n",
    "    # dummy points for legend\n",
    "    xmin, xmax = ax[1].get_xlim()\n",
    "    ymin, ymax = ax[1].get_ylim()\n",
    "    ax[1].plot(0, 0, 's', markersize=10, markerfacecolor=colors[0], markeredgecolor='k', markeredgewidth=0.5, label=\"No snow\")\n",
    "    ax[1].plot(0, 0, 's', markersize=10, markerfacecolor=colors[1], markeredgecolor='k', markeredgewidth=0.5, label=\"Snow\")\n",
    "    ax[1].plot([0,1], [0,1], '-k', linewidth=1.5, label=\"SLA\")\n",
    "    ax[1].set_xlim(xmin, xmax)\n",
    "    ax[1].set_ylim(ymin, ymax)\n",
    "    ax[1].legend(loc='upper left')\n",
    "\n",
    "    ax[1].set_xlabel(\"Easting [km]\")\n",
    "\n",
    "\n",
    "    # Histograms of all elevations and snow elevations with lines for SLA metrics\n",
    "    snow_dem = dem.where(classified_image==1)\n",
    "    bins = np.linspace(np.nanmin(dem.data), np.nanmax(dem.data), num=100)\n",
    "    ax[2].hist(dem.data.ravel(), bins=bins, color='gray', alpha=1, label=\"All elevations\")\n",
    "    ax[2].hist(snow_dem.data.ravel(), bins=bins, color=colors[1], alpha=1, label=\"Snow elevations\")\n",
    "    ax[2].axvline(sla_upper, color='k', linestyle='dashed', label=\"SLA$_{upper}$ = \"+str(int(sla_upper))+\" m\")\n",
    "    ax[2].axvline(sla, color='k', linestyle='solid', label=\"SLA = \"+str(int(sla))+\" m\")\n",
    "    ax[2].axvline(sla_lower, color='k', linestyle='dotted', label=\"SLA$_{lower}$ = \"+str(int(sla_lower))+\" m\")\n",
    "    ax[2].legend(loc='upper right')\n",
    "    ax[2].set_xlabel(\"Elevation [m]\")\n",
    "    ax[2].set_ylabel(\"Counts\")\n",
    "\n",
    "    fig.suptitle(f\"{date} {dataset}\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Save to file\n",
    "    fig.savefig(out_file, dpi=300, bbox_inches='tight')\n",
    "    print(\"Figure saved to:\", out_file)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81cb959-e6b6-46f3-bf59-b2186b4efbe4",
   "metadata": {},
   "source": [
    "## Run the workflow: query GEE for imagery, classify snow, calculate stats, plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcb1ebe-6027-4037-a8ad-d7feb7707949",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(out_folder, exist_ok=True)\n",
    "\n",
    "# Run each dataset separately\n",
    "# NOTE: Could also run for Sentinel-2_SR, but only available after 2019\n",
    "for dataset in [\"Sentinel-2_TOA\", \"Landsat\"]:\n",
    "    print(f\"\\n\\n{dataset}\\n--------------------\")\n",
    "\n",
    "    # Define dataset-specific variables\n",
    "    if \"Sentinel\" in dataset:\n",
    "        download_bands = [\"B4\", \"B3\", \"B2\", \"NDSI\"]\n",
    "        scale = 10\n",
    "    else:\n",
    "        download_bands = [\"SR_B4\", \"SR_B3\", \"SR_B2\", \"NDSI\"]\n",
    "        scale = 30\n",
    "    rgb_bands = download_bands[0:-1]\n",
    "\n",
    "    # Get daily date ranges\n",
    "    date_ranges = split_date_range_daily(dataset, date_start, date_end, month_start, month_end)\n",
    "\n",
    "    # Iterate over date ranges\n",
    "    for d_start, d_end in tqdm(date_ranges):\n",
    "        # Define outputs\n",
    "        settings_string = f\"NDSI{ndsi_threshold}_Blue{blue_threshold if type(blue_threshold)==float else 0}\"\n",
    "        image_file = os.path.join(out_folder, f\"{d_start}_{dataset}.tif\")\n",
    "        classified_image_file = os.path.join(out_folder, f\"{d_start}_{dataset}_classified_{settings_string}.tif\")\n",
    "        snow_stats_file = os.path.join(out_folder, f\"{d_start}_{dataset}_snow_stats_{settings_string}.csv\")\n",
    "        fig_file = os.path.join(out_folder, f\"{d_start}_{dataset}_results_{settings_string}.png\")\n",
    "\n",
    "        # Check if results have already been saved\n",
    "        if os.path.exists(fig_file) and os.path.exists(snow_stats_file):\n",
    "            print(f\"Files for {d_start} already exist. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Query GEE for imagery       \n",
    "        image_col = utils.query_gee_for_imagery(\n",
    "            dataset = dataset,\n",
    "            aoi = aoi,\n",
    "            date_start = str(d_start), \n",
    "            date_end = str(d_end), \n",
    "            fill_portion = min_aoi_coverage, \n",
    "            mask_clouds = mask_clouds, \n",
    "            scale = None, \n",
    "            verbose = False\n",
    "        )\n",
    "\n",
    "        # Check if any images were found\n",
    "        if image_col.size().getInfo() < 1:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n{d_start}\")\n",
    "\n",
    "        # Convert image to xarray.Dataset\n",
    "        image = image_col.first().select(download_bands)\n",
    "        image_xr = image.wx.to_xarray(region=aoi, scale=scale, crs=utm_crs)\n",
    "        image_xr = image_xr.squeeze(drop=True)\n",
    "        if download_images:\n",
    "            image_xr.rio.to_raster(image_file)\n",
    "            print(f\"Image saved to: {image_file}\")\n",
    "\n",
    "        # Classify snow\n",
    "        classified_image_xr = classify_snow(\n",
    "            image_xr = image_xr, \n",
    "            ndsi_threshold = ndsi_threshold,\n",
    "            blue_threshold = blue_threshold,\n",
    "            out_file = classified_image_file if download_images else None\n",
    "            )\n",
    "\n",
    "        # Estimate snowline elevation\n",
    "        snow_stats_df, dem_masked_xr = calculate_snowline_altitude(\n",
    "            date = d_start, \n",
    "            dataset = dataset, \n",
    "            classified_image = classified_image_xr, \n",
    "            dem = dem_xr, \n",
    "            sla_percentile = sla_percentile,\n",
    "            out_file = snow_stats_file\n",
    "        )\n",
    "        \n",
    "        # Plot results\n",
    "        plot_snow_cover_stats(\n",
    "            date = d_start,\n",
    "            dataset = dataset,\n",
    "            image = image_xr,\n",
    "            classified_image = classified_image_xr,\n",
    "            dem = dem_masked_xr,\n",
    "            snow_stats_df = snow_stats_df,\n",
    "            out_file = fig_file\n",
    "        )\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c8d83",
   "metadata": {},
   "source": [
    "## Optional: compile all snow cover stats into one CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f83d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# get files\n",
    "stats_files = sorted(glob(os.path.join(out_folder, \"*.csv\")))\n",
    "print(f\"Located {len(stats_files)} snow cover stats files\")\n",
    "\n",
    "# compile in dataframe\n",
    "df_list = []\n",
    "for file in stats_files:\n",
    "    df_list += [pd.read_csv(file)]\n",
    "compiled_stats_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# save to file\n",
    "compiled_stats_file = os.path.join(out_folder, \"compiled_snow_stats.csv\")\n",
    "compiled_stats_df.to_csv(compiled_stats_file, index=False)\n",
    "print(f\"Compiled snow cover stats saved to: {compiled_stats_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: remove individual files\n",
    "# commented out to precent accidents!\n",
    "\n",
    "# for file in stats_files:\n",
    "#     os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc784cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glasee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
